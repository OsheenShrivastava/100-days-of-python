# -*- coding: utf-8 -*-
"""Fatal_Force_(start).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bI0zLoobjhgp40OpXog2EjvlvuT41Yvj

# Introduction

Since Jan. 1, 2015, [The Washington Post](https://www.washingtonpost.com/) has been compiling a database of every fatal shooting in the US by a police officer in the line of duty.

<center><img src=https://i.imgur.com/sX3K62b.png></center>

While there are many challenges regarding data collection and reporting, The Washington Post has been tracking more than a dozen details about each killing. This includes the race, age and gender of the deceased, whether the person was armed, and whether the victim was experiencing a mental-health crisis. The Washington Post has gathered this supplemental information from law enforcement websites, local new reports, social media, and by monitoring independent databases such as "Killed by police" and "Fatal Encounters". The Post has also conducted additional reporting in many cases.

There are 4 additional datasets: US census data on poverty rate, high school graduation rate, median household income, and racial demographics. [Source of census data](https://factfinder.census.gov/faces/nav/jsf/pages/community_facts.xhtml).

### Upgrade Plotly

Run the cell below if you are working with Google Colab
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade plotly

"""## Import Statements"""

import numpy as np
import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

"""## Notebook Presentation"""

pd.options.display.float_format = '{:,.2f}'.format

"""## Load the Data"""

df_hh_income = pd.read_csv('Median_Household_Income_2015.csv', encoding="windows-1252")
df_pct_poverty = pd.read_csv('Pct_People_Below_Poverty_Level.csv', encoding="windows-1252")
df_pct_completed_hs = pd.read_csv('Pct_Over_25_Completed_High_School.csv', encoding="windows-1252")
df_share_race_city = pd.read_csv('Share_of_Race_By_City.csv', encoding="windows-1252")
df_fatalities = pd.read_csv('Deaths_by_Police_US.csv', encoding="windows-1252")

"""# Preliminary Data Exploration

* What is the shape of the DataFrames?
* How many rows and columns do they have?
* What are the column names?
* Are there any NaN values or duplicates?
"""

print(df_hh_income.shape)
print(df_pct_poverty.shape)
print(df_pct_completed_hs.shape)
print(df_share_race_city.shape)
print(df_fatalities.shape)

print(df_hh_income.columns)
print(df_pct_poverty.columns)
print(df_pct_completed_hs.columns)
print(df_share_race_city.columns)
print(df_fatalities.columns)

print(df_hh_income.isna().values.any())
print(df_pct_poverty.isna().values.any())
print(df_pct_completed_hs.isna().values.any())
print(df_share_race_city.isna().values.any())
print(df_fatalities.isna().values.any())

print(df_hh_income.duplicated().values.any())
print(df_pct_poverty.duplicated().values.any())
print(df_pct_completed_hs.duplicated().values.any())
print(df_share_race_city.duplicated().values.any())
print(df_fatalities.duplicated().values.any())

"""## Data Cleaning - Check for Missing Values and Duplicates

Consider how to deal with the NaN values. Perhaps substituting 0 is appropriate.
"""

df_hh_income.fillna(0, inplace=True)
print(df_hh_income)

df_fatalities.fillna(0, inplace=True)
print(df_fatalities)

"""# Chart the Poverty Rate in each US State

Create a bar chart that ranks the poverty rate from highest to lowest by US state. Which state has the highest poverty rate? Which state has the lowest poverty rate?  Bar Plot
"""

df_pct_poverty['poverty_rate'] = df_pct_poverty['poverty_rate'].replace("-", 0)

df_pct_poverty['poverty_rate'] = pd.to_numeric(df_pct_poverty['poverty_rate'])
print(df_pct_poverty.head())

df_pct_poverty_sorted = df_pct_poverty.sort_values("poverty_rate", ascending=False)
print(df_pct_poverty_sorted)

# If u want to use px.bar directly then aggregate the poverty_rate since
# consists duplicates i.e., group by Geographic Area and find mean.
plt.figure(figsize=(14, 6))
plt.bar(df_pct_poverty_sorted['Geographic Area'], df_pct_poverty_sorted['poverty_rate'])
plt.xticks(rotation=90)
plt.title("Poverty Rate by U.S. State (Highest to Lowest)")
plt.ylabel("Poverty Rate")
plt.show()

"""# Chart the High School Graduation Rate by US State

Show the High School Graduation Rate in ascending order of US States. Which state has the lowest high school graduation rate? Which state has the highest?
"""

df_pct_completed_hs['percent_completed_hs'] = df_pct_completed_hs['percent_completed_hs'].replace("-", 0)

df_pct_completed_hs['percent_completed_hs'] = pd.to_numeric(df_pct_completed_hs['percent_completed_hs'])
print(df_pct_completed_hs.head())

df_pct_completed_hs_sorted = df_pct_completed_hs.sort_values("percent_completed_hs")
print(df_pct_completed_hs_sorted)

plt.figure(figsize=(14, 6))
plt.bar(df_pct_completed_hs_sorted['Geographic Area'], df_pct_completed_hs_sorted['percent_completed_hs'])
plt.xticks(rotation=90)
plt.title("High School Graduation Rate by U.S. State (Lowest to Highest)")
plt.ylabel("High School Graduation Rate")
plt.show()

"""# Visualise the Relationship between Poverty Rates and High School Graduation Rates

#### Create a line chart with two y-axes to show if the rations of poverty and high school graduation move together.  
"""

# Take the average per state before merging
df_poverty_mean = df_pct_poverty_sorted.groupby('Geographic Area', as_index=False)['poverty_rate'].mean()
df_grad_mean = df_pct_completed_hs_sorted.groupby('Geographic Area', as_index=False)['percent_completed_hs'].mean()

# Merge the cleaned averages
# "on" tells pandas which column name to use as the common key when merging (joining) the two DataFrames.
df_merged = pd.merge(df_poverty_mean, df_grad_mean, on='Geographic Area').sort_values(by='poverty_rate')


plt.figure(figsize=(14, 7), dpi=200)
plt.title("Relationship Between Poverty Rate and High School Graduation Rate", fontsize=18)
plt.xticks(rotation=90, fontsize=12)
plt.yticks(fontsize=12)

ax1 = plt.gca()
ax2 = ax1.twinx()

ax1.plot(df_merged["Geographic Area"], df_merged["poverty_rate"],
         color="crimson", linewidth=3, label="Poverty Rate (%)")

ax2.plot(df_merged["Geographic Area"], df_merged["percent_completed_hs"],
         color="dodgerblue", linewidth=3, label="High School Graduation Rate (%)")

ax1.set_xlabel("U.S. State", fontsize=14)
ax1.set_ylabel("Poverty Rate (%)", color="crimson", fontsize=14)
ax2.set_ylabel("Graduation Rate (%)", color="dodgerblue", fontsize=14)

ax1.legend(loc="upper left")
ax2.legend(loc="upper right")

plt.tight_layout()
plt.show()

"""#### Now use a Seaborn .jointplot() with a Kernel Density Estimate (KDE) and/or scatter plot to visualise the same relationship"""

sns.set(style="white", font_scale=1.2)

sns.jointplot(
    data=df_merged,
    x='poverty_rate',
    y='percent_completed_hs',
    kind='kde',
    fill=True,
    cmap="coolwarm",
    height=8,
)
plt.suptitle("Relationship Between Poverty Rate and High School Graduation Rate",
             fontsize=16, y=1.02)
plt.xlabel("Poverty Rate (%)")
plt.ylabel("High School Graduation Rate (%)")
plt.show()

sns.set(style="whitegrid", font_scale=1.2)

# Create a simple scatter jointplot
sns.jointplot(
    data=df_merged,
    x="poverty_rate",
    y="percent_completed_hs",
    kind="scatter",
    color="dodgerblue",
    height=8
)

plt.suptitle(
    "Relationship Between Poverty Rate and High School Graduation Rate",
    fontsize=16,
    y=1.02
)
plt.xlabel("Poverty Rate (%)")
plt.ylabel("High School Graduation Rate (%)")
plt.show()

"""#### Seaborn's `.lmplot()` or `.regplot()` to show a linear regression between the poverty ratio and the high school graduation ratio."""

with sns.axes_style("whitegrid"):
  sns.lmplot(data=df_merged,
             x='poverty_rate',
             y='percent_completed_hs',
             hue=None,
             aspect=2,
             scatter_kws = {'alpha': 0.4, 'color':'blue'},
             line_kws={'color': 'black', 'linewidth':2})

plt.title("Poverty Rate vs High School Graduation Rate")
plt.xlabel("Poverty Rate (%)")
plt.ylabel("High School Graduation Rate (%)")
plt.show()

with sns.axes_style("whitegrid"):
  sns.regplot(
    data=df_merged,
    x='poverty_rate',
    y='percent_completed_hs',
    scatter_kws={'alpha':0.4, 'color':'blue'},
    line_kws={'color':'black', 'linewidth':2}
)

plt.title("Linear Regression: Poverty Rate vs High School Graduation Rate")
plt.xlabel("Poverty Rate (%)")
plt.ylabel("High School Graduation Rate (%)")
plt.show()

"""# Create a Bar Chart with Subsections Showing the Racial Makeup of Each US State

Visualise the share of the white, black, hispanic, asian and native american population in each US State using a bar chart with sub sections.
"""

# Define columns
cols = ['share_white', 'share_black', 'share_hispanic', 'share_asian', 'share_native_american']

# Clean values: remove %, (X), —, and convert to numeric
for col in cols:
    df_share_race_city[col] = (df_share_race_city[col].replace({'%':'', r'\(X\)':'', '-':'', '—':'', '': '0'}, regex=True))
    df_share_race_city[col] = pd.to_numeric(df_share_race_city[col], errors='coerce').fillna(0)

# Calculate total for sorting
df_share_race_city['Total'] = df_share_race_city[cols].sum(axis=1)
df_share_race_city.sort_values(by='Total', ascending=False, inplace=True)

# Create a long-format DataFrame without using melt
df_long = pd.concat([df_share_race_city[['Geographic area', col]].rename(columns={col: 'Population (%)'}).assign(Race=col.replace('share_', '').replace('_', ' ').title())
    for col in cols
])

# Horizontal stacked bar chart
fig = px.bar(
    df_long,
    x='Population (%)',
    y='Geographic area',
    color='Race',
    orientation='h',
    title='Racial Makeup of Each US State',
    labels={'Population (%)': 'Population (%)', 'State': 'US State'}
)

fig.update_layout(
    barmode='stack',
    yaxis={'categoryorder': 'total ascending'},
    legend_title_text='Race'
)

fig.show()

"""# Create Donut Chart by of People Killed by Race

Hint: Use `.value_counts()`
"""

# Define race columns (make sure names match exactly)
race_cols = ['share_white', 'share_black', 'share_native_american', 'share_asian', 'share_hispanic']

# Keep only those that actually exist
race_cols = [col for col in race_cols if col in df_share_race_city.columns]
print("Using columns:", race_cols)

# Convert them to numeric safely
for col in race_cols:
    df_share_race_city[col] = pd.to_numeric(df_share_race_city[col], errors='coerce')

# Compute mean of each race column
race_mean = df_share_race_city[race_cols].mean(numeric_only=True)

print("\nRace mean values:\n", race_mean)

# Convert Series → DataFrame
race_df = race_mean.reset_index()
race_df.columns = ['Race', 'Share']

# Create donut chart
fig = px.pie(
    race_df,
    names='Race',
    values='Share',
    hole=0.5,
    title='Average Racial Composition Across All Cities',
    color_discrete_sequence=px.colors.qualitative.Pastel
)

fig.update_traces(textinfo='percent+label')
fig.show()

"""# Create a Chart Comparing the Total Number of Deaths of Men and Women

Use `df_fatalities` to illustrate how many more men are killed compared to women.
"""

# Count number of fatalities by gender
gender_counts = df_fatalities['gender'].value_counts().reset_index()
gender_counts.columns = ['Gender', 'Count']
print(gender_counts)

# Create bar chart
fig = px.bar(
    gender_counts,
    x='Gender',
    y='Count',
    color='Gender',
    text='Count',
    title='Total Number of Deaths by Gender',
    color_discrete_sequence=px.colors.qualitative.Pastel
)

fig.update_traces(textposition='outside')
fig.update_layout(showlegend=False)
fig.show()

"""# Create a Box Plot Showing the Age and Manner of Death

Break out the data by gender using `df_fatalities`. Is there a difference between men and women in the manner of death?
"""

plt.figure(figsize=(8,4), dpi=200)
with sns.axes_style("whitegrid"):
  sns.boxplot(data=df_fatalities,
              x='manner_of_death',
              y='age',
              hue='gender',
              palette='pastel')

plt.show()

"""# Were People Armed?

In what percentage of police killings were people armed? Create chart that show what kind of weapon (if any) the deceased was carrying. How many of the people killed by police were armed with guns versus unarmed?
"""

killing_counts = df_fatalities['armed'].value_counts().reset_index()
killing_counts.columns = ['armed', 'Count']
killing_counts['Count'] = pd.to_numeric(killing_counts['Count'], errors='coerce')
killing_counts['Percentage'] = (killing_counts['Count'] / killing_counts['Count'].sum()) * 100
print(killing_counts)

# Create bar chart
fig = px.bar(
    killing_counts,
    x='Percentage',
    y='armed',
    color='armed',
    text=killing_counts['Percentage'].round(1).astype(str) + '%',
    title='Percentage of Police Killings by Weapon Type',
    color_discrete_sequence=px.colors.qualitative.Pastel
)

fig.update_traces(textposition='outside')
fig.update_layout(showlegend=False)
fig.show()

"""# How Old Were the People Killed?

Work out what percentage of people killed were under 25 years old.
"""

number_of_people_killed_under_25 = len(df_fatalities[df_fatalities['age'] < 25])
total_killed = len(df_fatalities)
under_25_percentage = (number_of_people_killed_under_25 / total_killed) * 100
print(f"{under_25_percentage:.2f}")

"""Create a histogram and KDE plot that shows the distribution of ages of the people killed by police."""

plt.figure(figsize=(8,4), dpi=200)
sns.histplot(data=df_fatalities, x='age', bins=30, kde=True)
plt.title('Distribution of Ages of People Killed by Police', fontsize=14)
plt.xlabel('Age', fontsize=12)
plt.ylabel('Number of People', fontsize=12)
plt.show()

"""Create a seperate KDE plot for each race. Is there a difference between the distributions?"""

plt.figure(figsize=(10,6), dpi=150)
sns.kdeplot(
    data=df_fatalities,
    x='age',
    hue='race',          # separate line per race
    fill=False,          # keep lines only for clarity
    common_norm=False,   # don’t normalize together — keep real proportions
    linewidth=2
)
plt.show()

"""# Race of People Killed

Create a chart that shows the total number of people killed by race.
"""

# Replace 'race' with your actual column name if different
race_counts = (df_fatalities['race'].value_counts().reset_index())
race_counts.columns = ['Race', 'Count']
print(race_counts)

# Plotly bar chart
fig = px.bar(
    race_counts,
    x='Race',
    y='Count',
    text='Count',
    color='Race',
    title='Total Number of People Killed by Race',
    color_discrete_sequence=px.colors.qualitative.Pastel
)

fig.update_traces(textposition='outside')
fig.update_layout(
    xaxis_title='Race',
    yaxis_title='Number of People Killed',
    showlegend=False
)
fig.show()

"""# Mental Illness and Police Killings

What percentage of people killed by police have been diagnosed with a mental illness?
"""

mental_illness_count = df_fatalities['signs_of_mental_illness'].value_counts()
print(mental_illness_count)
people_killed = (mental_illness_count.get(True, 0) / mental_illness_count.sum()) * 100
print(f"{people_killed:.2f}")

"""# In Which Cities Do the Most Police Killings Take Place?

Create a chart ranking the top 10 cities with the most police killings. Which cities are the most dangerous?  
"""

killings_by_city = df_fatalities['city'].value_counts().reset_index()
killings_by_city.columns = ['City', 'Count']
print(killings_by_city)

top_10_cities = killings_by_city.head(10)
print(top_10_cities)

fig = px.bar(
    top_10_cities,  # sort for clean horizontal bars
    x='Count',
    y='City',
    orientation='h',
    text='Count',
    color='City',
    title='Top 10 Cities with the Most Police Killings (Ranked)',
    color_discrete_sequence=px.colors.qualitative.Pastel
)

fig.update_traces(textposition='outside')
fig.update_layout(
    showlegend=False,
    yaxis_title='City',
    xaxis_title='Number of People Killed',
    yaxis={'categoryorder': 'total ascending'}
)
fig.show()

"""# Rate of Death by Race

Find the share of each race in the top 10 cities. Contrast this with the top 10 cities of police killings to work out the rate at which people are killed by race for each city.
"""

# Filter fatalities to only include top 10 cities
top_cities_fatalities = df_fatalities[df_fatalities['city'].isin(top_10_cities['City'])]
# print(top_cities_fatalities)

# Count killings by race in each top city
killings_by_race_city = top_cities_fatalities.groupby(['city', 'race']).size().reset_index(name='Killings')
print(killings_by_race_city)

# Get total killings per city for rate calculation
total_killings_per_city  = top_cities_fatalities.groupby('city').size().reset_index(name='Total_Killings')
print(total_killings_per_city)

# Merge to calculate killing rate per race
killings_by_race_city = killings_by_race_city.merge(total_killings_per_city, on='city')
print(killings_by_race_city)

killings_by_race_city['Killing_Rate'] = killings_by_race_city['Killings'] / killings_by_race_city['Total_Killings']
print(killings_by_race_city)

"""# Create a Choropleth Map of Police Killings by US State

Which states are the most dangerous? Compare your map with your previous chart. Are these the same states with high degrees of poverty?
"""

state_counts = df_fatalities['state'].value_counts().reset_index()
state_counts.columns = ['state', 'killings']
print(state_counts)

state_to_abbrev = {
 "ALABAMA":"AL","ALASKA":"AK","ARIZONA":"AZ","ARKANSAS":"AR","CALIFORNIA":"CA","COLORADO":"CO",
 "CONNECTICUT":"CT","DELAWARE":"DE","FLORIDA":"FL","GEORGIA":"GA","HAWAII":"HI","IDAHO":"ID",
 "ILLINOIS":"IL","INDIANA":"IN","IOWA":"IA","KANSAS":"KS","KENTUCKY":"KY","LOUISIANA":"LA",
 "MAINE":"ME","MARYLAND":"MD","MASSACHUSETTS":"MA","MICHIGAN":"MI","MINNESOTA":"MN","MISSISSIPPI":"MS",
 "MISSOURI":"MO","MONTANA":"MT","NEBRASKA":"NE","NEVADA":"NV","NEW HAMPSHIRE":"NH","NEW JERSEY":"NJ",
 "NEW MEXICO":"NM","NEW YORK":"NY","NORTH CAROLINA":"NC","NORTH DAKOTA":"ND","OHIO":"OH","OKLAHOMA":"OK",
 "OREGON":"OR","PENNSYLVANIA":"PA","RHODE ISLAND":"RI","SOUTH CAROLINA":"SC","SOUTH DAKOTA":"SD",
 "TENNESSEE":"TN","TEXAS":"TX","UTAH":"UT","VERMONT":"VT","VIRGINIA":"VA","WASHINGTON":"WA",
 "WEST VIRGINIA":"WV","WISCONSIN":"WI","WYOMING":"WY","DISTRICT OF COLUMBIA":"DC"
}

sc = state_counts.copy()
# if values look like names (len>2) map them to abbreviations
if sc['state'].str.len().max() > 2:
    sc['state_code'] = sc['state'].str.strip().str.upper().map(state_to_abbrev)
else:
    sc['state_code'] = sc['state']

sc = sc.dropna(subset=['state_code'])

# choropleth
fig = px.choropleth(
    sc,
    locations='state_code',
    locationmode="USA-states",
    color='killings',
    scope="usa",
    labels={'killings':'Police Killings'},
    title="Police Killings by US State (from provided CSV)"
)
fig.update_layout(margin={"r":0,"t":40,"l":0,"b":0})
fig.show()

# bar chart of the top 15 states
top_bar = state_counts.head(15)
bar_fig = px.bar(top_bar, x=top_bar.iloc[:,0], y='killings', title="Top 15 states by police killings")
bar_fig.show()

"""# Number of Police Killings Over Time

Analyse the Number of Police Killings over Time. Is there a trend in the data?
"""

df_fatalities['date'] = pd.to_datetime(df_fatalities['date'], errors='coerce')
df_fatalities['year'] = df_fatalities['date'].dt.year

print(df_fatalities)

killings_by_year = df_fatalities.groupby('year').size().reset_index(name='killings')
print(killings_by_year)

fig = px.line(
    killings_by_year,
    x='year',
    y='killings',
    markers=True,
    title='Number of Police Killings Over Time (US)',
    labels={'year': 'Year', 'killings': 'Number of Killings'}
)
fig.update_layout(xaxis=dict(dtick=1))
fig.show()

"""# Epilogue

Now that you have analysed the data yourself, read [The Washington Post's analysis here](https://www.washingtonpost.com/graphics/investigations/police-shootings-database/).
"""